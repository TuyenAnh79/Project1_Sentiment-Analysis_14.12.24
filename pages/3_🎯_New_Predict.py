import pickle
import streamlit as st
import pandas as pd
import pickle
from utils import *
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.linear_model import LogisticRegression
from sklearn.metrics import classification_report


st.set_page_config(page_title="New Predict", page_icon="üéØ")

# Sidebar
st.sidebar.success("Gi√°o Vi√™n H∆∞·ªõng D·∫´n: \n # KHU·∫§T THU·ª≤ PH∆Ø∆†NG")
st.sidebar.success("H·ªçc Vi√™n:\n # NGUY·ªÑN CH·∫§N NAM \n # CH·∫æ TH·ªä ANH TUY·ªÄN")
st.sidebar.success("Ng√†y b√°o c√°o: \n # 16/12/2024")

################################ LOAD C√ÅC FILE H·ªñ TR·ª¢ VI·ªÜC PREPROCESSING DATA

# Load TEENCODE v√† s·∫Øp x·∫øp theo chi·ªÅu d√†i key gi·∫£m d·∫ßn
teen_dict = load_from_file('files/teencode.txt')
teen_dict = dict(sorted(teen_dict.items(), key=lambda item: len(item[0]), reverse=True))

# Load EMOJICON
emoji_dict = load_from_file('files/emojicon.txt')

# Load STOPWORDS
stopwords_dict = load_from_file('files/vietnamese-stopwords.txt')
stopwords_lst = list(stopwords_dict.keys())

# Load WRONG WORDS v√† t·ª± ƒë·ªông lo·∫°i b·ªè tr√πng l·∫∑p
wrong_dict = load_from_file('files/wrong-word.txt')
wrong_lst=list(wrong_dict.keys())

# Load measurement_unit
measurement_unit = load_from_file('files/measurement_unit.txt')
#-------

# Load tu_ghep_dict
tu_ghep_dict = load_and_process_file('files/tu_ghep.txt')

# Load dong_san_pham_dict
dong_san_pham_dict = load_and_process_file('files/dong_san_pham.txt')

# Load brand_dict
brand_dict = load_and_process_file('files/brand.txt')

# Load POSITIVE_WORDS
positive_words = load_and_process_file('files/positive_words_VN.txt')

# Load NEGATIVE_WORDS
negative_words = load_and_process_file('files/negative_words_VN.txt')


################################ D·ª∞ ƒêO√ÅN GI√Å TR·ªä M·ªöI


# H√†m d·ª± ƒëo√°n gi√° tr·ªã m·ªõi cho c√°c b√¨nh lu·∫≠n
def predict_new_data_with_probabilities(model, new_texts, vectorizer):
    
    # Chuy·ªÉn ƒë·ªïi vƒÉn b·∫£n m·ªõi th√†nh vector
    new_texts_transformed = vectorizer.transform(new_texts)
    
    # D·ª± ƒëo√°n x√°c su·∫•t cho c√°c l·ªõp (positive v√† negative)
    probabilities = model.predict_proba(new_texts_transformed)
    
    # D·ª± ƒëo√°n nh√£n cu·ªëi c√πng
    predictions = model.predict(new_texts_transformed)
    
    # K·∫øt qu·∫£: X√°c su·∫•t v√† nh√£n d·ª± ƒëo√°n
    results = []
    for text, prob, pred in zip(new_texts, probabilities, predictions):
        sentiment = "positive" if pred == 1 else "negative"
        results.append({
            "B√¨nh lu·∫≠n": text,
            "X√°c su·∫•t Positive": round(prob[1], 4),  # X√°c su·∫•t l·ªõp 1
            "X√°c su·∫•t Negative": round(prob[0], 4),  # X√°c su·∫•t l·ªõp 0
            "K·∫øt qu·∫£": sentiment  # K·∫øt qu·∫£ cu·ªëi c√πng
        })
    return results


# T·∫£i m√¥ h√¨nh v√† vectorizer t·ª´ c√°c file pickle
model_filename = 'logistic_regression_model.pkl'  
vectorizer_filename = 'logistic_regression_vectorizer.pkl'


# ·ª®ng d·ª•ng Streamlit
st.title("üîÆ Sentiment Prediction")
st.write("D·ª± ƒëo√°n c·∫£m x√∫c (t√≠ch c·ª±c/ti√™u c·ª±c) c·ªßa b√¨nh lu·∫≠n d·ª±a tr√™n m√¥ h√¨nh h·ªçc m√°y.")

try:
    # T·∫£i m√¥ h√¨nh LOGISTIC REGRESSION v√† vectorizer t·ª´ file
    with open(model_filename, 'rb') as f:
        model = pickle.load(f)
    with open(vectorizer_filename, 'rb') as f:
        vectorizer = pickle.load(f)

    # Giao di·ªán nh·∫≠p d·ªØ li·ªáu
    st.subheader("üìù Nh·∫≠p b√¨nh lu·∫≠n ƒë·ªÉ d·ª± ƒëo√°n:")

    # Ch·ªçn ph∆∞∆°ng th·ª©c nh·∫≠p li·ªáu: nh·∫≠p tay ho·∫∑c t·∫£i file
    option = st.radio("Ch·ªçn ph∆∞∆°ng ph√°p nh·∫≠p li·ªáu:", ["Nh·∫≠p vƒÉn b·∫£n tr·ª±c ti·∫øp", "T·∫£i file vƒÉn b·∫£n (.txt ho·∫∑c .csv)"])

    # Bi·∫øn ƒë·ªÉ l∆∞u danh s√°ch b√¨nh lu·∫≠n
    comments = []

    if option == "Nh·∫≠p vƒÉn b·∫£n tr·ª±c ti·∫øp":
        # Nh·∫≠p vƒÉn b·∫£n
        new_text = st.text_area("Nh·∫≠p b√¨nh lu·∫≠n, m·ªói b√¨nh lu·∫≠n tr√™n m·ªôt d√≤ng:")

        st.markdown('**N·ªôi dung b√¨nh lu·∫≠n m·∫´u:**')
        st.markdown('"ƒê√£ mua ƒë·ªß m√†u, r·ªìi ƒë·ªïi sp kh√°c nh∆∞ng v·∫´n ph·∫£i quay l·∫°i v·ªõi m√†u H·ªìng, d√πng m√†u H·ªìng ƒëi mn ∆°i, r·∫•t m·ªÅm m·ªãn da, d√πng 2-3 ng√†y th·∫•y ngay kh√°c bi·ªát"')

        if new_text.strip():
            comments = new_text.splitlines()

    elif option == "T·∫£i file vƒÉn b·∫£n (.txt ho·∫∑c .csv)":
        uploaded_file = st.file_uploader("T·∫£i file b√¨nh lu·∫≠n (.txt ho·∫∑c .csv):", type=['txt', 'csv'])
        if uploaded_file is not None:
            if uploaded_file.name.endswith('.txt'):
                # ƒê·ªçc file .txt
                content = uploaded_file.read().decode('utf-8')
                comments = content.splitlines()
            elif uploaded_file.name.endswith('.csv'):
                # ƒê·ªçc file .csv
                df = pd.read_csv(uploaded_file)
                if 'comment' in df.columns:  # Ki·ªÉm tra c·ªôt d·ªØ li·ªáu
                    comments = df['comment'].dropna().tolist()
                else:
                    st.error("File CSV c·∫ßn c√≥ c·ªôt t√™n 'comment' ch·ª©a n·ªôi dung b√¨nh lu·∫≠n!")

    if st.button("üéØ D·ª± ƒëo√°n"):
        if comments:
            try:
                # Ti·ªÅn x·ª≠ l√Ω c√°c b√¨nh lu·∫≠n
                preprocessed_comments = [
                    preprocessing(comment, brand_dict, teen_dict, emoji_dict, dong_san_pham_dict,
                                  positive_words, negative_words, tu_ghep_dict, special_words, stopwords_lst)
                    for comment in comments
                ]
            
                # Bi·∫øn ƒë·ªïi d·ªØ li·ªáu m·ªõi b·∫±ng vectorizer ƒë√£ fit
                X_new = vectorizer.transform(preprocessed_comments)

                # D·ª± ƒëo√°n k·∫øt qu·∫£ v√† x√°c su·∫•t
                predictions = model.predict(X_new)
                probabilities = model.predict_proba(X_new)

                # T·∫°o DataFrame k·∫øt qu·∫£
                results_df = pd.DataFrame({
                    'B√¨nh lu·∫≠n': comments,
                    'D·ª± ƒëo√°n': predictions,
                    'X√°c su·∫•t L·ªõp 0': probabilities[:, 0],
                    'X√°c su·∫•t L·ªõp 1': probabilities[:, 1]
                })

                # Hi·ªÉn th·ªã k·∫øt qu·∫£ d∆∞·ªõi d·∫°ng b·∫£ng
                st.subheader("üîç K·∫øt qu·∫£ D·ª± ƒëo√°n:")
                st.table(results_df)

                # Cho ph√©p t·∫£i file k·∫øt qu·∫£ v·ªÅ
                csv = results_df.to_csv(index=False).encode('utf-8')
                st.download_button(
                    label="üíΩ T·∫£i k·∫øt qu·∫£ d·ª± ƒëo√°n v·ªÅ file CSV",
                    data=csv,
                    file_name="sentiment_predictions_with_results.csv",
                    mime="text/csv"
                )
            except Exception as e:
                st.error(f"ƒê√£ x·∫£y ra l·ªói khi x·ª≠ l√Ω b√¨nh lu·∫≠n: {e}")
        else:
            st.warning("Vui l√≤ng nh·∫≠p b√¨nh lu·∫≠n ho·∫∑c t·∫£i file ƒë·ªÉ d·ª± ƒëo√°n!")

except FileNotFoundError:
    st.error(f"Kh√¥ng t√¨m th·∫•y file '{model_filename}' ho·∫∑c '{vectorizer_filename}'. Vui l√≤ng ki·ªÉm tra l·∫°i ƒë∆∞·ªùng d·∫´n file.")
except Exception as e:
    st.error(f"ƒê√£ x·∫£y ra l·ªói: {e}")
